{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pydot) (2.2.0)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11138103851034954918, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 10173376139222016140\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'saved_models/LSTM/5epoch_test.h5'\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 348.00 337.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 344,-333 344,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140606081261640 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140606081261640</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 161,-328.5 161,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-306.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606081262760 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140606081262760</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140606081261640&#45;&gt;140606081262760 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140606081261640-&gt;140606081262760</title>\n",
       "<path d=\"M80.5,-292.4551C80.5,-284.3828 80.5,-274.6764 80.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-265.5903 80.5,-255.5904 77.0001,-265.5904 84.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081261696 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140606081261696</title>\n",
       "<polygon fill=\"none\" points=\"179,-219.5 179,-255.5 340,-255.5 340,-219.5 179,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606081263208 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140606081263208</title>\n",
       "<polygon fill=\"none\" points=\"171,-146.5 171,-182.5 332,-182.5 332,-146.5 171,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140606081261696&#45;&gt;140606081263208 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140606081261696-&gt;140606081263208</title>\n",
       "<path d=\"M257.5225,-219.4551C256.6378,-211.3828 255.5741,-201.6764 254.5884,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"258.0511,-192.1495 253.4825,-182.5904 251.0928,-192.9122 258.0511,-192.1495\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081262424 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140606081262424</title>\n",
       "<polygon fill=\"none\" points=\"46.5,-146.5 46.5,-182.5 144.5,-182.5 144.5,-146.5 46.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140606081262760&#45;&gt;140606081262424 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140606081262760-&gt;140606081262424</title>\n",
       "<path d=\"M84.2079,-219.4551C85.8846,-211.2951 87.9044,-201.4652 89.7694,-192.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"93.1984,-193.0902 91.7828,-182.5904 86.3416,-191.6812 93.1984,-193.0902\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081262592 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140606081262592</title>\n",
       "<polygon fill=\"none\" points=\"120.5,-73.5 120.5,-109.5 218.5,-109.5 218.5,-73.5 120.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 140606081263208&#45;&gt;140606081262592 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140606081263208-&gt;140606081262592</title>\n",
       "<path d=\"M231.2303,-146.4551C221.0788,-137.4177 208.6262,-126.3319 197.5709,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"199.617,-113.6255 189.8207,-109.5904 194.9625,-118.8539 199.617,-113.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081262424&#45;&gt;140606081262592 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140606081262424-&gt;140606081262592</title>\n",
       "<path d=\"M113.7921,-146.4551C122.8644,-137.5054 133.9729,-126.547 143.8768,-116.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.5008,-119.1049 151.1618,-109.5904 141.5848,-114.1215 146.5008,-119.1049\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081263712 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140606081263712</title>\n",
       "<polygon fill=\"none\" points=\"118.5,-.5 118.5,-36.5 220.5,-36.5 220.5,-.5 118.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140606081262592&#45;&gt;140606081263712 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140606081262592-&gt;140606081263712</title>\n",
       "<path d=\"M169.5,-73.4551C169.5,-65.3828 169.5,-55.6764 169.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"173.0001,-46.5903 169.5,-36.5904 166.0001,-46.5904 173.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 18, 256)      1699328     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 17, 256)      2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1024), (None 5246976     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 17, 1024), ( 5246976     embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17, 8778)     8997450     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,437,898\n",
      "Trainable params: 23,437,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define encoder\n",
    "encoder_model = Model(inputs=model.input[0], #encoder_input\n",
    "                      outputs=model.get_layer('lstm_1').output[1:]) #enconder lstm hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 169.00 191.00\" width=\"169pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 165,-187 165,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140606081261640 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140606081261640</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606081262760 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140606081262760</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 161,-109.5 161,-73.5 0,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-87.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140606081261640&#45;&gt;140606081262760 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140606081261640-&gt;140606081262760</title>\n",
       "<path d=\"M80.5,-146.4551C80.5,-138.3828 80.5,-128.6764 80.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-119.5903 80.5,-109.5904 77.0001,-119.5904 84.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081262424 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140606081262424</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-.5 31.5,-36.5 129.5,-36.5 129.5,-.5 31.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-14.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140606081262760&#45;&gt;140606081262424 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140606081262760-&gt;140606081262424</title>\n",
       "<path d=\"M80.5,-73.4551C80.5,-65.3828 80.5,-55.6764 80.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-46.5903 80.5,-36.5904 77.0001,-46.5904 84.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_tar_size = model.get_layer('dense_1').weights[1].shape.as_list()[0]\n",
    "\n",
    "decoder_word_input = Input(shape=(1,),name='decoder_input')\n",
    "decoder_input_embedding = Embedding(input_dim=vocab_tar_size, \n",
    "                                    output_dim=embedding_dim,\n",
    "                                    weights=model.get_layer('embedding_2').get_weights())(decoder_word_input)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(units,), name='decoder_input_h')\n",
    "decoder_state_input_c = Input(shape=(units,), name='decoder_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm = LSTM(units, \n",
    "                    return_sequences=False, \n",
    "                    return_state=True,\n",
    "                    weights=model.get_layer('lstm_2').get_weights())\n",
    "decoder_output, state_h, state_c = decoder_lstm(decoder_input_embedding,\n",
    "                                                initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_dense = Dense(vocab_tar_size, \n",
    "                      activation='softmax',\n",
    "                      weights=model.get_layer('dense_1').get_weights())\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_word_input] + decoder_states_inputs,\n",
    "                      outputs=[decoder_output] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 554.50 264.00\" width=\"555pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 550.5,-260 550.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140606077913296 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140606077913296</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606078357456 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140606078357456</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140606077913296&#45;&gt;140606078357456 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140606077913296-&gt;140606078357456</title>\n",
       "<path d=\"M80.5,-219.4551C80.5,-211.3828 80.5,-201.6764 80.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-192.5903 80.5,-182.5904 77.0001,-192.5904 84.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606081210912 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140606081210912</title>\n",
       "<polygon fill=\"none\" points=\"217.5,-73.5 217.5,-109.5 315.5,-109.5 315.5,-73.5 217.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140606078357456&#45;&gt;140606081210912 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140606078357456-&gt;140606081210912</title>\n",
       "<path d=\"M126.4775,-146.4551C152.0752,-136.4087 184.1226,-123.8309 211.0963,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"212.3767,-116.5019 220.4068,-109.5904 209.8193,-109.9858 212.3767,-116.5019\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606078322672 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140606078322672</title>\n",
       "<polygon fill=\"none\" points=\"179,-146.5 179,-182.5 354,-182.5 354,-146.5 179,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-160.8\">decoder_input_h: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606078322672&#45;&gt;140606081210912 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140606078322672-&gt;140606081210912</title>\n",
       "<path d=\"M266.5,-146.4551C266.5,-138.3828 266.5,-128.6764 266.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-119.5903 266.5,-109.5904 263.0001,-119.5904 270.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606077725776 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140606077725776</title>\n",
       "<polygon fill=\"none\" points=\"372.5,-146.5 372.5,-182.5 546.5,-182.5 546.5,-146.5 372.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-160.8\">decoder_input_c: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140606077725776&#45;&gt;140606081210912 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140606077725776-&gt;140606081210912</title>\n",
       "<path d=\"M411.7921,-146.4551C385.2311,-136.4087 351.9776,-123.8309 323.9888,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"324.9195,-109.8545 314.3279,-109.5904 322.443,-116.4019 324.9195,-109.8545\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140606077412128 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140606077412128</title>\n",
       "<polygon fill=\"none\" points=\"215.5,-.5 215.5,-36.5 317.5,-36.5 317.5,-.5 215.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140606081210912&#45;&gt;140606077412128 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140606081210912-&gt;140606077412128</title>\n",
       "<path d=\"M266.5,-73.4551C266.5,-65.3828 266.5,-55.6764 266.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-46.5903 266.5,-36.5904 263.0001,-46.5904 270.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 256)       2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1024), (None 5246976     embedding_1[0][0]                \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8778)         8997450     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,491,594\n",
      "Trainable params: 16,491,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニコードファイルを ascii に変換\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 文の開始と終了のトークンを付加\n",
    "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    with open(path) as f:\n",
    "        word_pairs = f.readlines()\n",
    "    word_pairs = [preprocess_sentence(sentence) for sentence in word_pairs]\n",
    "\n",
    "    return word_pairs[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he thought irritably . <end>\n",
      "<start> 彼 は いらだ ち ながら 思 っ た 。 <end>\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "path_train_en = 'small_parallel_enja/train.en'\n",
    "path_train_ja = 'small_parallel_enja/train.ja'\n",
    "en = create_dataset(path_train_en, None)\n",
    "ja = create_dataset(path_train_ja, None)\n",
    "print(en[-1])\n",
    "print(ja[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    #これはなくす。\n",
    "    #学習済みw2vをembeddingにつかうので\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # クリーニングされた入力と出力のペアを生成\n",
    "    lang = create_dataset(path, num_examples)\n",
    "\n",
    "    tensor, lang_tokenizer = tokenize(lang)\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語へのIDの割り振りとID列への変換\n",
    "# このサイズのデータセットで実験\n",
    "num_examples = None\n",
    "input_tensor, inp_lang = load_dataset(path_train_en, num_examples)\n",
    "target_tensor, targ_lang = load_dataset(path_train_ja, num_examples)\n",
    "# ターゲットテンソルの最大長を計算\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# 80-20で分割を行い、訓練用と検証用のデータセットを作成\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 長さを表示\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "8 ----> you\n",
      "42 ----> can\n",
      "20 ----> 't\n",
      "73 ----> see\n",
      "41 ----> him\n",
      "351 ----> because\n",
      "10 ----> he\n",
      "9 ----> is\n",
      "1285 ----> engaged\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "15 ----> 彼\n",
      "5 ----> は\n",
      "67 ----> 仕事\n",
      "94 ----> 中\n",
      "13 ----> な\n",
      "10 ----> の\n",
      "12 ----> で\n",
      "3900 ----> 面会\n",
      "5 ----> は\n",
      "253 ----> 出来\n",
      "21 ----> ま\n",
      "41 ----> せ\n",
      "30 ----> ん\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 42.,   5.,  46.,   9.,  16.,  11.,   6.,  21.,  19.,  22.,   4.,\n",
       "          3.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 15.,   5.,  27.,  67.,   9., 265.,   8.,  10.,  12.,  19.,  22.,\n",
       "          4.,   3.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 15.,  10.,  65.,   7.,   5.,  46.,  51.,  22.,  10., 325.,  14.,\n",
       "          6.,  21.,  19.,   4.,   3.,   0.,   0.],\n",
       "       [ 18.,   5.,  15.,   7.,  82.,  20.,   8.,  34.,   9., 501.,  11.,\n",
       "          6.,  17.,   4.,   3.,   0.,   0.,   0.],\n",
       "       [ 15.,   5.,  27.,  67.,   9., 265.,   8.,   4.,   3.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_sequence(input_seq, targ_lang, max_length_targ):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    vocab_tar_size = np.array(list(targ_lang.index_word.keys())).max()\n",
    "    inp_batch_size = len(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((inp_batch_size, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[:, 0] = targ_lang.word_index['<start>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = np.zeros((inp_batch_size, max_length_targ))\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens,axis=1) #array of size [inp_batch_size, 1]\n",
    "\n",
    "        decoded_sentence[:,i] = sampled_token_index\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((inp_batch_size, 1))\n",
    "        target_seq[:, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence    \n",
    "\n",
    "decode_sequence(input_tensor_val[0:5],targ_lang,max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sentence(seq,lang):\n",
    "    def index2lang(idx, lang):\n",
    "        try:\n",
    "            return lang.index_word[idx]\n",
    "        except KeyError:\n",
    "            return ''\n",
    "    langseq2sentence = np.vectorize(lambda x: index2lang(x,lang),otypes=[str])\n",
    "    sentences = langseq2sentence(seq)\n",
    "    sentences = [' '.join(list(sentence)) for sentence in sentences]\n",
    "    sentences = [sentence.lstrip('<start>').strip(' ').strip('<end>') for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may i have an ice bag ? ',\n",
       " 'ken has been to england before . ',\n",
       " 'there is a conference going on in the next room . ',\n",
       " 'i hear he is looking for work . ',\n",
       " \"he makes it a rule to go to bed at eleven o 'clock . \"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input = seq2sentence(input_tensor_val[0:5],inp_lang)\n",
    "val_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あなた は 何 を し て い ま す か 。 ',\n",
       " '彼 は その 仕事 を 終え た の で す か 。 ',\n",
       " '彼 の 家 に は 何 人 か の 生徒 が い ま す 。 ',\n",
       " '私 は 彼 に 会 っ た こと を 覚え て い る 。 ',\n",
       " '彼 は その 仕事 を 終え た 。 ']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = seq2sentence(decode_sequence(input_tensor_val[0:5],targ_lang,max_length_targ),targ_lang)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['氷のう を くださ い 。 ',\n",
       " '健 は 以前 イングランド に 行 っ た こと が あ り ま す 。 ',\n",
       " '隣 の 部屋 で は 会議 中 で す 。 ',\n",
       " '彼 は 勤め 口 を 探 し て い る そう だ 。 ',\n",
       " '彼 は １１ 時 に 床 に つ く こと に し て い る 。 ']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = seq2sentence(target_tensor_val[0:5],targ_lang)\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLUE score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def calc_BLEU(pred_l,test_l):\n",
    "    score = [sentence_bleu([reference], candidate) for reference, candidate in zip(test_l,pred_l)]\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12495961246035017"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_BLEU(prediction,val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
