{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pydot) (2.2.0)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15419060170660111321, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 10585813761493709623\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'saved_models/LSTM/5epoch_test.h5'\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 348.00 337.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 344,-333 344,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139835317858544 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139835317858544</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 161,-328.5 161,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-306.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835317859496 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139835317859496</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 139835317858544&#45;&gt;139835317859496 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139835317858544-&gt;139835317859496</title>\n",
       "<path d=\"M80.5,-292.4551C80.5,-284.3828 80.5,-274.6764 80.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-265.5903 80.5,-255.5904 77.0001,-265.5904 84.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317858656 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139835317858656</title>\n",
       "<polygon fill=\"none\" points=\"179,-219.5 179,-255.5 340,-255.5 340,-219.5 179,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835317859216 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139835317859216</title>\n",
       "<polygon fill=\"none\" points=\"171,-146.5 171,-182.5 332,-182.5 332,-146.5 171,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 139835317858656&#45;&gt;139835317859216 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139835317858656-&gt;139835317859216</title>\n",
       "<path d=\"M257.5225,-219.4551C256.6378,-211.3828 255.5741,-201.6764 254.5884,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"258.0511,-192.1495 253.4825,-182.5904 251.0928,-192.9122 258.0511,-192.1495\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317859272 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139835317859272</title>\n",
       "<polygon fill=\"none\" points=\"46.5,-146.5 46.5,-182.5 144.5,-182.5 144.5,-146.5 46.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139835317859496&#45;&gt;139835317859272 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139835317859496-&gt;139835317859272</title>\n",
       "<path d=\"M84.2079,-219.4551C85.8846,-211.2951 87.9044,-201.4652 89.7694,-192.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"93.1984,-193.0902 91.7828,-182.5904 86.3416,-191.6812 93.1984,-193.0902\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317859440 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139835317859440</title>\n",
       "<polygon fill=\"none\" points=\"120.5,-73.5 120.5,-109.5 218.5,-109.5 218.5,-73.5 120.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 139835317859216&#45;&gt;139835317859440 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139835317859216-&gt;139835317859440</title>\n",
       "<path d=\"M231.2303,-146.4551C221.0788,-137.4177 208.6262,-126.3319 197.5709,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"199.617,-113.6255 189.8207,-109.5904 194.9625,-118.8539 199.617,-113.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317859272&#45;&gt;139835317859440 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139835317859272-&gt;139835317859440</title>\n",
       "<path d=\"M113.7921,-146.4551C122.8644,-137.5054 133.9729,-126.547 143.8768,-116.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.5008,-119.1049 151.1618,-109.5904 141.5848,-114.1215 146.5008,-119.1049\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317860392 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139835317860392</title>\n",
       "<polygon fill=\"none\" points=\"118.5,-.5 118.5,-36.5 220.5,-36.5 220.5,-.5 118.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139835317859440&#45;&gt;139835317860392 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139835317859440-&gt;139835317860392</title>\n",
       "<path d=\"M169.5,-73.4551C169.5,-65.3828 169.5,-55.6764 169.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"173.0001,-46.5903 169.5,-36.5904 166.0001,-46.5904 173.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 18, 256)      1699328     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 17, 256)      2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1024), (None 5246976     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 17, 1024), ( 5246976     embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17, 8778)     8997450     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,437,898\n",
      "Trainable params: 23,437,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define encoder\n",
    "encoder_model = Model(inputs=model.input[0], #encoder_input\n",
    "                      outputs=model.get_layer('lstm_1').output[1:]) #enconder lstm hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 169.00 191.00\" width=\"169pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 165,-187 165,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139835317858544 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139835317858544</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835317859496 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139835317859496</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 161,-109.5 161,-73.5 0,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-87.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 139835317858544&#45;&gt;139835317859496 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139835317858544-&gt;139835317859496</title>\n",
       "<path d=\"M80.5,-146.4551C80.5,-138.3828 80.5,-128.6764 80.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-119.5903 80.5,-109.5904 77.0001,-119.5904 84.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835317859272 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139835317859272</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-.5 31.5,-36.5 129.5,-36.5 129.5,-.5 31.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-14.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139835317859496&#45;&gt;139835317859272 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139835317859496-&gt;139835317859272</title>\n",
       "<path d=\"M80.5,-73.4551C80.5,-65.3828 80.5,-55.6764 80.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-46.5903 80.5,-36.5904 77.0001,-46.5904 84.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_tar_size = model.get_layer('dense_1').weights[1].shape.as_list()[0]\n",
    "\n",
    "decoder_word_input = Input(shape=(1,),name='decoder_input')\n",
    "decoder_input_embedding = Embedding(input_dim=vocab_tar_size, \n",
    "                                    output_dim=embedding_dim,\n",
    "                                    weights=model.get_layer('embedding_2').get_weights())(decoder_word_input)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(units,), name='decoder_input_h')\n",
    "decoder_state_input_c = Input(shape=(units,), name='decoder_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm = LSTM(units, \n",
    "                    return_sequences=False, \n",
    "                    return_state=True,\n",
    "                    weights=model.get_layer('lstm_2').get_weights())\n",
    "decoder_output, state_h, state_c = decoder_lstm(decoder_input_embedding,\n",
    "                                                initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_dense = Dense(vocab_tar_size, \n",
    "                      activation='softmax',\n",
    "                      weights=model.get_layer('dense_1').get_weights())\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_word_input] + decoder_states_inputs,\n",
    "                      outputs=[decoder_output] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 554.50 264.00\" width=\"555pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 550.5,-260 550.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139835277870080 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139835277870080</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835283708000 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139835283708000</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 139835277870080&#45;&gt;139835283708000 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139835277870080-&gt;139835283708000</title>\n",
       "<path d=\"M80.5,-219.4551C80.5,-211.3828 80.5,-201.6764 80.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-192.5903 80.5,-182.5904 77.0001,-192.5904 84.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835277872656 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139835277872656</title>\n",
       "<polygon fill=\"none\" points=\"217.5,-73.5 217.5,-109.5 315.5,-109.5 315.5,-73.5 217.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 139835283708000&#45;&gt;139835277872656 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139835283708000-&gt;139835277872656</title>\n",
       "<path d=\"M126.4775,-146.4551C152.0752,-136.4087 184.1226,-123.8309 211.0963,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"212.3767,-116.5019 220.4068,-109.5904 209.8193,-109.9858 212.3767,-116.5019\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835278539240 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139835278539240</title>\n",
       "<polygon fill=\"none\" points=\"179,-146.5 179,-182.5 354,-182.5 354,-146.5 179,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-160.8\">decoder_input_h: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835278539240&#45;&gt;139835277872656 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139835278539240-&gt;139835277872656</title>\n",
       "<path d=\"M266.5,-146.4551C266.5,-138.3828 266.5,-128.6764 266.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-119.5903 266.5,-109.5904 263.0001,-119.5904 270.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835283708728 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139835283708728</title>\n",
       "<polygon fill=\"none\" points=\"372.5,-146.5 372.5,-182.5 546.5,-182.5 546.5,-146.5 372.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-160.8\">decoder_input_c: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139835283708728&#45;&gt;139835277872656 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139835283708728-&gt;139835277872656</title>\n",
       "<path d=\"M411.7921,-146.4551C385.2311,-136.4087 351.9776,-123.8309 323.9888,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"324.9195,-109.8545 314.3279,-109.5904 322.443,-116.4019 324.9195,-109.8545\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139835283803552 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139835283803552</title>\n",
       "<polygon fill=\"none\" points=\"215.5,-.5 215.5,-36.5 317.5,-36.5 317.5,-.5 215.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 139835277872656&#45;&gt;139835283803552 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139835277872656-&gt;139835283803552</title>\n",
       "<path d=\"M266.5,-73.4551C266.5,-65.3828 266.5,-55.6764 266.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-46.5903 266.5,-36.5904 263.0001,-46.5904 270.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 256)       2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 1024), (None 5246976     embedding_2[0][0]                \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8778)         8997450     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,491,594\n",
      "Trainable params: 16,491,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニコードファイルを ascii に変換\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 文の開始と終了のトークンを付加\n",
    "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    with open(path) as f:\n",
    "        word_pairs = f.readlines()\n",
    "    word_pairs = [preprocess_sentence(sentence) for sentence in word_pairs]\n",
    "\n",
    "    return word_pairs[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he thought irritably . <end>\n",
      "<start> 彼 は いらだ ち ながら 思 っ た 。 <end>\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "path_train_en = 'small_parallel_enja/train.en'\n",
    "path_train_ja = 'small_parallel_enja/train.ja'\n",
    "en = create_dataset(path_train_en, None)\n",
    "ja = create_dataset(path_train_ja, None)\n",
    "print(en[-1])\n",
    "print(ja[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    #これはなくす。\n",
    "    #学習済みw2vをembeddingにつかうので\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # クリーニングされた入力と出力のペアを生成\n",
    "    lang = create_dataset(path, num_examples)\n",
    "\n",
    "    tensor, lang_tokenizer = tokenize(lang)\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語へのIDの割り振りとID列への変換\n",
    "# このサイズのデータセットで実験\n",
    "num_examples = None\n",
    "input_tensor, inp_lang = load_dataset(path_train_en, num_examples)\n",
    "target_tensor, targ_lang = load_dataset(path_train_ja, num_examples)\n",
    "# ターゲットテンソルの最大長を計算\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# 80-20で分割を行い、訓練用と検証用のデータセットを作成\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 長さを表示\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "7 ----> to\n",
      "522 ----> understand\n",
      "599 ----> someone\n",
      "9 ----> is\n",
      "7 ----> to\n",
      "297 ----> love\n",
      "599 ----> someone\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "303 ----> だれ\n",
      "22 ----> か\n",
      "9 ----> を\n",
      "461 ----> 理解\n",
      "39 ----> する\n",
      "34 ----> こと\n",
      "5 ----> は\n",
      "31 ----> 、\n",
      "27 ----> その\n",
      "51 ----> 人\n",
      "9 ----> を\n",
      "353 ----> 愛\n",
      "39 ----> する\n",
      "34 ----> こと\n",
      "23 ----> だ\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 42.,   5.,  46.,   9.,  16.,  11.,   6.,  21.,  19.,  22.,   4.,\n",
       "          3.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 18.,   5.,  42.,  10., 161.,   7., 390.,  12.,  19.,   4.,   3.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 15.,   5.,  27.,  67.,   9., 265.,   8.,   4.,   3.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 24.,   5.,  27.,  67.,   9., 265.,   8.,   4.,   3.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 15.,  10.,  65.,   5.,  76., 189.,  43.,   8.,   4.,   3.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_sequence(input_seq, targ_lang, max_length_targ):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    vocab_tar_size = np.array(list(targ_lang.index_word.keys())).max()\n",
    "    inp_batch_size = len(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((inp_batch_size, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[:, 0] = targ_lang.word_index['<start>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = np.zeros((inp_batch_size, max_length_targ))\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens,axis=1) #array of size [inp_batch_size, 1]\n",
    "\n",
    "        decoded_sentence[:,i] = sampled_token_index\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((inp_batch_size, 1))\n",
    "        target_seq[:, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence    \n",
    "\n",
    "decode_sequence(input_tensor_val[0:5],targ_lang,max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(range(10)):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor_val[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang.word_index['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang.word_index['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang.index_word[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ----> <start>\n",
      "19 ----> have\n",
      "8 ----> you\n",
      "262 ----> finished\n",
      "664 ----> writing\n",
      "36 ----> your\n",
      "5557 ----> thesis\n",
      "12 ----> ?\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "convert(inp_lang, input_tensor_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ----> <start>\n",
      "2050 ----> 論文\n",
      "9 ----> を\n",
      "143 ----> 書\n",
      "37 ----> き\n",
      "265 ----> 終え\n",
      "21 ----> ま\n",
      "16 ----> し\n",
      "8 ----> た\n",
      "22 ----> か\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "convert(targ_lang, target_tensor_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要に応じて修正\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
