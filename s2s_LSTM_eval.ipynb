{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要\n",
    "学習済みLSTMモデルの評価を行う。\n",
    "* モデルを使った英→日の翻訳を行う\n",
    "* 与えられた文章のBLEUスコアの平均値を算出する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前提パッケージのインストール\n",
    "モデルの可視化のため、pydotをインストールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pydot) (2.2.0)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "終了後一度kernel restartする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュール、モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15554071161273717340, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 6507052368661346304\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'saved_models/LSTM/20epochs_LSTM.h5'\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 348.00 337.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 344,-333 344,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140523683270328 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140523683270328</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 161,-328.5 161,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-306.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523682809224 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140523682809224</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140523683270328&#45;&gt;140523682809224 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140523683270328-&gt;140523682809224</title>\n",
       "<path d=\"M80.5,-292.4551C80.5,-284.3828 80.5,-274.6764 80.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-265.5903 80.5,-255.5904 77.0001,-265.5904 84.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682808776 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140523682808776</title>\n",
       "<polygon fill=\"none\" points=\"179,-219.5 179,-255.5 340,-255.5 340,-219.5 179,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523682808440 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140523682808440</title>\n",
       "<polygon fill=\"none\" points=\"171,-146.5 171,-182.5 332,-182.5 332,-146.5 171,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"251.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 140523682808776&#45;&gt;140523682808440 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140523682808776-&gt;140523682808440</title>\n",
       "<path d=\"M257.5225,-219.4551C256.6378,-211.3828 255.5741,-201.6764 254.5884,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"258.0511,-192.1495 253.4825,-182.5904 251.0928,-192.9122 258.0511,-192.1495\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682808608 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140523682808608</title>\n",
       "<polygon fill=\"none\" points=\"46.5,-146.5 46.5,-182.5 144.5,-182.5 144.5,-146.5 46.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140523682809224&#45;&gt;140523682808608 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140523682809224-&gt;140523682808608</title>\n",
       "<path d=\"M84.2079,-219.4551C85.8846,-211.2951 87.9044,-201.4652 89.7694,-192.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"93.1984,-193.0902 91.7828,-182.5904 86.3416,-191.6812 93.1984,-193.0902\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682809056 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140523682809056</title>\n",
       "<polygon fill=\"none\" points=\"120.5,-73.5 120.5,-109.5 218.5,-109.5 218.5,-73.5 120.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 140523682808440&#45;&gt;140523682809056 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140523682808440-&gt;140523682809056</title>\n",
       "<path d=\"M231.2303,-146.4551C221.0788,-137.4177 208.6262,-126.3319 197.5709,-116.4899\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"199.617,-113.6255 189.8207,-109.5904 194.9625,-118.8539 199.617,-113.6255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682808608&#45;&gt;140523682809056 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140523682808608-&gt;140523682809056</title>\n",
       "<path d=\"M113.7921,-146.4551C122.8644,-137.5054 133.9729,-126.547 143.8768,-116.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.5008,-119.1049 151.1618,-109.5904 141.5848,-114.1215 146.5008,-119.1049\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682810064 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140523682810064</title>\n",
       "<polygon fill=\"none\" points=\"118.5,-.5 118.5,-36.5 220.5,-36.5 220.5,-.5 118.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140523682809056&#45;&gt;140523682810064 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140523682809056-&gt;140523682810064</title>\n",
       "<path d=\"M169.5,-73.4551C169.5,-65.3828 169.5,-55.6764 169.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"173.0001,-46.5903 169.5,-36.5904 166.0001,-46.5904 173.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 18, 256)      1699328     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 17, 256)      2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1024), (None 5246976     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 17, 1024), ( 5246976     embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17, 8778)     8997450     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,437,898\n",
      "Trainable params: 23,437,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルはLSTMを使ったseq2seqモデル。\n",
    "\n",
    "入力を256次元にEmbeddingし、LSTMによってhidden states(h,c)にエンコーディングする。\n",
    "\n",
    "エンコードされたhidden states(h,c)をデコーダーのLSTMに渡し、LSTM出力をDense+softmaxで単語ごとの確率に変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論モデルの構築\n",
    "seq2seq学習時はt番目の正解単語を使ってt+1番目の単語を予測させていた(teacher forcing)。\n",
    "\n",
    "推論時はt番目の出力単語を使ってt+1番目の単語を予測する必要があるので、データの流れが異なる。\n",
    "\n",
    "このノートでは、学習時に得た重みを使った別のモデルを定義することでこの流れを実現している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論用encoderの定義\n",
    "encoderは学習時も推論時も同様の振る舞いをするため、学習時のモデルがそのまま流用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define encoder\n",
    "encoder_model = Model(inputs=model.input[0], #encoder_input\n",
    "                      outputs=model.get_layer('lstm_1').output[1:]) #enconder lstm hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 169.00 191.00\" width=\"169pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-187 165,-187 165,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140523683270328 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140523683270328</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">encoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523682809224 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140523682809224</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 161,-109.5 161,-73.5 0,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-87.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140523683270328&#45;&gt;140523682809224 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140523683270328-&gt;140523682809224</title>\n",
       "<path d=\"M80.5,-146.4551C80.5,-138.3828 80.5,-128.6764 80.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-119.5903 80.5,-109.5904 77.0001,-119.5904 84.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523682808608 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140523682808608</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-.5 31.5,-36.5 129.5,-36.5 129.5,-.5 31.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-14.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140523682809224&#45;&gt;140523682808608 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140523682809224-&gt;140523682808608</title>\n",
       "<path d=\"M80.5,-73.4551C80.5,-65.3828 80.5,-55.6764 80.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-46.5903 80.5,-36.5904 77.0001,-46.5904 84.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論用decoderの定義\n",
    "decoderはteacher forcingをしないので定義を変更する必要がある。\n",
    "\n",
    "大まかに、以下の流れで処理を行う。\n",
    "1. 入力された1単語をembedding_dim次元にembedding\n",
    "2. さらに入力としてunits次元の2つのベクトルdecoder_state_input_h, decoder_state_input_cを受け取る\n",
    "3. embedding, decoder_state_input_h, decoder_state_input_cをLSTMに入力する\n",
    "4. LSTMのoutputをDense+softmaxで出力単語ごとの確率に変換する\n",
    "5. モデルのアウトプットに出力単語ごとの確率、LSTMのstate_h, state_cを設定\n",
    "\n",
    "embedding, LSTM, Denseにはそれぞれ学習済みの重みを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_tar_size = model.get_layer('dense_1').weights[1].shape.as_list()[0]\n",
    "\n",
    "decoder_word_input = Input(shape=(1,),name='decoder_input')\n",
    "decoder_input_embedding = Embedding(input_dim=vocab_tar_size, \n",
    "                                    output_dim=embedding_dim,\n",
    "                                    weights=model.get_layer('embedding_2').get_weights())(decoder_word_input)\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(units,), name='decoder_input_h')\n",
    "decoder_state_input_c = Input(shape=(units,), name='decoder_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm = LSTM(units, \n",
    "                    return_sequences=False, \n",
    "                    return_state=True,\n",
    "                    weights=model.get_layer('lstm_2').get_weights())\n",
    "decoder_output, state_h, state_c = decoder_lstm(decoder_input_embedding,\n",
    "                                                initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_dense = Dense(vocab_tar_size, \n",
    "                      activation='softmax',\n",
    "                      weights=model.get_layer('dense_1').get_weights())\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_word_input] + decoder_states_inputs,\n",
    "                      outputs=[decoder_output] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 554.50 264.00\" width=\"555pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 550.5,-260 550.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140523209048528 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140523209048528</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">decoder_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523209048472 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140523209048472</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 161,-182.5 161,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140523209048528&#45;&gt;140523209048472 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140523209048528-&gt;140523209048472</title>\n",
       "<path d=\"M80.5,-219.4551C80.5,-211.3828 80.5,-201.6764 80.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"84.0001,-192.5903 80.5,-182.5904 77.0001,-192.5904 84.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523209048360 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140523209048360</title>\n",
       "<polygon fill=\"none\" points=\"217.5,-73.5 217.5,-109.5 315.5,-109.5 315.5,-73.5 217.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140523209048472&#45;&gt;140523209048360 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140523209048472-&gt;140523209048360</title>\n",
       "<path d=\"M126.4775,-146.4551C152.0752,-136.4087 184.1226,-123.8309 211.0963,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"212.3767,-116.5019 220.4068,-109.5904 209.8193,-109.9858 212.3767,-116.5019\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523209048304 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140523209048304</title>\n",
       "<polygon fill=\"none\" points=\"179,-146.5 179,-182.5 354,-182.5 354,-146.5 179,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-160.8\">decoder_input_h: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523209048304&#45;&gt;140523209048360 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140523209048304-&gt;140523209048360</title>\n",
       "<path d=\"M266.5,-146.4551C266.5,-138.3828 266.5,-128.6764 266.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-119.5903 266.5,-109.5904 263.0001,-119.5904 270.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523209131960 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140523209131960</title>\n",
       "<polygon fill=\"none\" points=\"372.5,-146.5 372.5,-182.5 546.5,-182.5 546.5,-146.5 372.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-160.8\">decoder_input_c: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140523209131960&#45;&gt;140523209048360 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140523209131960-&gt;140523209048360</title>\n",
       "<path d=\"M411.7921,-146.4551C385.2311,-136.4087 351.9776,-123.8309 323.9888,-113.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"324.9195,-109.8545 314.3279,-109.5904 322.443,-116.4019 324.9195,-109.8545\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140523208976648 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140523208976648</title>\n",
       "<polygon fill=\"none\" points=\"215.5,-.5 215.5,-36.5 317.5,-36.5 317.5,-.5 215.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140523209048360&#45;&gt;140523208976648 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140523209048360-&gt;140523208976648</title>\n",
       "<path d=\"M266.5,-73.4551C266.5,-65.3828 266.5,-55.6764 266.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"270.0001,-46.5903 266.5,-36.5904 263.0001,-46.5904 270.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 256)       2247168     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 1024), (None 5246976     embedding_1[0][0]                \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8778)         8997450     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,491,594\n",
      "Trainable params: 16,491,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価用コーパスの用意\n",
    "trainingと同じ処理でコーパスを前処理する。ただしtrainでTokenizerを作り、testの文章を読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニコードファイルを ascii に変換\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 文の開始と終了のトークンを付加\n",
    "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    with open(path) as f:\n",
    "        word_pairs = f.readlines()\n",
    "    word_pairs = [preprocess_sentence(sentence) for sentence in word_pairs]\n",
    "\n",
    "    return word_pairs[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he thought irritably . <end>\n",
      "<start> 彼 は いらだ ち ながら 思 っ た 。 <end>\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "path_train_en = 'small_parallel_enja/train.en'\n",
    "path_train_ja = 'small_parallel_enja/train.ja'\n",
    "en = create_dataset(path_train_en, None)\n",
    "ja = create_dataset(path_train_ja, None)\n",
    "print(en[-1])\n",
    "print(ja[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    #これはなくす。\n",
    "    #学習済みw2vをembeddingにつかうので\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # クリーニングされた入力と出力のペアを生成\n",
    "    lang = create_dataset(path, num_examples)\n",
    "\n",
    "    tensor, lang_tokenizer = tokenize(lang)\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語へのIDの割り振りとID列への変換\n",
    "# このサイズのデータセットで実験\n",
    "num_examples = None\n",
    "input_tensor, inp_lang = load_dataset(path_train_en, num_examples)\n",
    "target_tensor, targ_lang = load_dataset(path_train_ja, num_examples)\n",
    "# ターゲットテンソルの最大長を計算\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> where do you come from ? <end>\n",
      "<start> どこ の ご 出身 で す か 。 <end>\n"
     ]
    }
   ],
   "source": [
    "# testデータの読み込み\n",
    "path_test_en = 'small_parallel_enja/test.en'\n",
    "path_test_ja = 'small_parallel_enja/test.ja'\n",
    "en_test = create_dataset(path_test_en, None)\n",
    "ja_test = create_dataset(path_test_ja, None)\n",
    "print(en_test[-1])\n",
    "print(ja_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp_tensor = inp_lang.texts_to_sequences(en_test)\n",
    "test_inp_tensor = keras.preprocessing.sequence.pad_sequences(test_inp_tensor, padding='post')\n",
    "\n",
    "test_targ_tensor = targ_lang.texts_to_sequences(ja_test)\n",
    "test_targ_tensor = keras.preprocessing.sequence.pad_sequences(test_targ_tensor, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "54 ----> they\n",
      "780 ----> finally\n",
      "2450 ----> acknowledged\n",
      "14 ----> it\n",
      "39 ----> as\n",
      "201 ----> true\n",
      "4 ----> .\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "2 ----> <start>\n",
      "15 ----> 彼\n",
      "33 ----> ら\n",
      "5 ----> は\n",
      "1113 ----> つい\n",
      "7 ----> に\n",
      "49 ----> それ\n",
      "14 ----> が\n",
      "518 ----> 真実\n",
      "23 ----> だ\n",
      "25 ----> と\n",
      "861 ----> 認め\n",
      "8 ----> た\n",
      "4 ----> 。\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, test_inp_tensor[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, test_targ_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 翻訳を行う関数の定義\n",
    "構築した推論モデルを使って実際に翻訳を行う関数を定義する。処理の流れの概要は以下\n",
    "1. encoderで入力文章をhidden statesに変換\n",
    "2. 文章開始トークンのword_indexとhidden statesをdecoderに入力し、2番目の単語のword_indexと更新されたhidden statesを得る\n",
    "3. 得られたword_indexをdecoded_sentenceに記録する\n",
    "4. max_length_targに達するまでword_indexとhidden statesをdecoderに入力し、decoded_sentenceを得る\n",
    "5. decoded_sentenceに記録されたword_indexを単語に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15.,   33.,    5.,   27.,  121.,    9.,  482.,    6.,    8.,\n",
       "           4.,    3.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [  15.,    5.,  392.,  570.,    7.,    5., 1307.,   16.,   13.,\n",
       "          43.,    8.,    4.,    3.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [  15.,    5.,   40.,  465.,  126.,    7., 1070.,   33.,  218.,\n",
       "         411.,   14.,    6.,    6.,    4.,    3.,    0.,    0.,    0.],\n",
       "       [ 188.,   71.,   79.,    7.,    5.,  174.,   33.,   13.,  110.,\n",
       "          53.,   13.,   33.,   13.,    6.,    4.,    3.,    0.,    0.],\n",
       "       [ 785.,    9.,  800.,    6.,   13.,   52.,    6.,    4.,    3.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_sequence(input_seq, targ_lang, max_length_targ):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    vocab_tar_size = np.array(list(targ_lang.index_word.keys())).max()\n",
    "    inp_batch_size = len(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((inp_batch_size, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[:, 0] = targ_lang.word_index['<start>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    decoded_sentence = np.zeros((inp_batch_size, max_length_targ))\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens,axis=1) #array of size [inp_batch_size, 1]\n",
    "\n",
    "        decoded_sentence[:,i] = sampled_token_index\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((inp_batch_size, 1))\n",
    "        target_seq[:, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence    \n",
    "\n",
    "decode_sequence(test_inp_tensor[0:5],targ_lang,max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded_sentenseのword_indexを単語に変換し、開始・終了トークンを取り除く\n",
    "def seq2sentence(seq,lang):\n",
    "    def index2lang(idx, lang):\n",
    "        try:\n",
    "            return lang.index_word[idx]\n",
    "        except KeyError:\n",
    "            return ''\n",
    "    langseq2sentence = np.vectorize(lambda x: index2lang(x,lang),otypes=[str])\n",
    "    sentences = langseq2sentence(seq)\n",
    "    sentences = [' '.join(list(sentence)) for sentence in sentences]\n",
    "    sentences = [sentence.lstrip('<start>').strip(' ').strip('<end>') for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they finally acknowledged it as true . ',\n",
       " \"he didn 't care for swimming . \",\n",
       " 'he is no less kind than his sister . ',\n",
       " 'you must be back before ten . ',\n",
       " 'break a leg . ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input = seq2sentence(test_inp_tensor[0:5],inp_lang)\n",
    "val_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼 ら は その 問題 を 解 い た 。 ',\n",
       " '彼 は 泳 ぐ に は 想像 し な かっ た 。 ',\n",
       " '彼 は お 姉 さん に 劣 ら ず 頭 が い い 。 ',\n",
       " '１０ 時 まで に は 帰 ら な けれ ば な ら な い 。 ',\n",
       " '足 を 洗 い な さ い 。 ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = seq2sentence(decode_sequence(test_inp_tensor[0:5],targ_lang,max_length_targ),targ_lang)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼 ら は つい に それ が 真実 だ と 認め た 。 ',\n",
       " '彼 は 水泳 が 得意 で は な かっ た 。 ',\n",
       " '彼 は お 姉 さん に 劣 ら ず 親切 だ 。 ',\n",
       " '１０ 時 前 に 戻 ら な けれ ば な ら な い 。 ',\n",
       " '成功 を 祈 る わ 。 ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = seq2sentence(test_targ_tensor[0:5],targ_lang)\n",
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEUスコア算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLUE score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def calc_BLEU(pred_l,test_l):\n",
    "    score = [sentence_bleu([reference], candidate) for reference, candidate in zip(test_l,pred_l)]\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3807680679420746"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_corpus_BLEU(test_inp,test_targ,batch_size):\n",
    "    schedule_idx = [[i*batch_size, (i+1)*batch_size]for i in range(divmod(len(test_inp),batch_size)[0])]\n",
    "    if divmod(len(test_inp),batch_size)[1] != 0:\n",
    "        schedule_idx += [[divmod(len(test_inp),batch_size)[0]*batch_size,len(test_inp)]]\n",
    "        \n",
    "    scores = []\n",
    "    for start_idx, end_idx in schedule_idx:\n",
    "        prediction = seq2sentence(decode_sequence(test_inp_tensor[start_idx:end_idx],targ_lang,max_length_targ),targ_lang)\n",
    "        target = seq2sentence(test_targ_tensor[start_idx:end_idx],targ_lang)\n",
    "        score = calc_BLEU(prediction,target)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "cal_corpus_BLEU(test_inp_tensor, test_targ_tensor,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMでのBLEUは0.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
